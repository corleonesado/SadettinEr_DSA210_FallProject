from __future__ import annotations

from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Optional, Tuple
import csv
import ssl
import time

import certifi
import cloudscraper
from bs4 import BeautifulSoup

BASE_URL = "https://www.forexfactory.com/calendar"
OUTPUT_CSV = Path("/tmp/forexfactory_high_impact_6m.csv")
REQUEST_DELAY_SECONDS = 1.2
WEEK_COUNT = 26
WEEK_OFFSET = 0

SSL_CONTEXT = ssl.create_default_context(cafile=certifi.where())


def get_last_6_months_weeks(
    limit: int = WEEK_COUNT, *, offset: int = WEEK_OFFSET, base_url: str = BASE_URL
) -> List[Tuple[datetime, str]]:
    """Return the requested number of (week_start_date, url) pairs."""
    weeks: List[Tuple[datetime, str]] = []
    today = datetime.now()
    current_monday = today - timedelta(days=today.weekday())

    for i in range(offset, offset + limit):
        target_date = current_monday - timedelta(weeks=i)
        date_param = target_date.strftime("%b%d.%Y").lower()
        url = f"{base_url}?week={date_param}"
        weeks.append((target_date, url))

    return weeks


def parse_day_breaker(label: str, reference: datetime, last_seen: Optional[datetime]) -> Optional[datetime]:
    """Turn strings like 'Mon Nov 24' into datetimes, rolling the year forward if needed."""
    label = label.strip()
    if not label:
        return last_seen

    try:
        candidate = datetime.strptime(f"{label} {reference.year}", "%a %b %d %Y")
    except ValueError:
        return last_seen

    if last_seen and candidate < last_seen - timedelta(days=2):
        candidate = candidate.replace(year=candidate.year + 1)

    return candidate


def is_high_impact(row) -> bool:
    """Return True when the row contains a red 'high impact' icon."""
    icon = row.select_one(".calendar__impact span.icon")
    if not icon:
        return False
    classes = " ".join(icon.get("class", []))
    return "impact-red" in classes


def clean_text(row, selector: str, default: str = "") -> str:
    node = row.select_one(selector)
    if not node:
        return default
    text = node.get_text(" ", strip=True)
    return text if text else default


def fetch_week(url: str, week_start: datetime, scraper) -> List[dict]:
    resp = scraper.get(url, timeout=30)
    if resp.status_code != 200:
        raise RuntimeError(f"HTTP {resp.status_code}")

    soup = BeautifulSoup(resp.text, "html.parser")
    table = soup.select_one("table.calendar__table")
    if not table:
        raise RuntimeError("Calendar table missing (blocked by Cloudflare?)")

    events: List[dict] = []
    last_seen_date: Optional[datetime] = None
    current_date: Optional[datetime] = None
    last_seen_time: str = "All Day"

    for row in table.select("tr.calendar__row"):
        classes = row.get("class", [])
        if "calendar__row--day-breaker" in classes:
            parsed = parse_day_breaker(row.get_text(" ", strip=True), week_start, last_seen_date)
            if parsed:
                current_date = parsed
                last_seen_date = parsed
                last_seen_time = "All Day"
            continue

        if not row.get("data-event-id") or not current_date:
            continue
        if not is_high_impact(row):
            continue

        time_text = clean_text(row, ".calendar__time", default="")
        if time_text:
            last_seen_time = time_text
        elif "calendar__row--no-grid" in classes:
            time_text = last_seen_time
        else:
            time_text = "All Day"
        events.append(
            {
                "event_id": row.get("data-event-id"),
                "date": current_date.strftime("%Y-%m-%d"),
                "time": time_text,
                "currency": clean_text(row, ".calendar__currency"),
                "event": clean_text(row, ".calendar__event-title"),
                "actual": clean_text(row, ".calendar__actual"),
                "forecast": clean_text(row, ".calendar__forecast"),
                "previous": clean_text(row, ".calendar__previous"),
            }
        )

    return events


def scrape_high_impact_events(*, week_count: int = WEEK_COUNT, week_offset: int = WEEK_OFFSET) -> List[dict]:
    weeks = get_last_6_months_weeks(limit=week_count, offset=week_offset)
    scraper = cloudscraper.create_scraper(
        browser={"browser": "chrome", "platform": "windows", "mobile": False},
        ssl_context=SSL_CONTEXT,
    )
    collected: List[dict] = []

    print(f"Configured to fetch {len(weeks)} week(s) starting offset {week_offset}.")
    for week_start, url in weeks:
        print(f"Fetching week starting {week_start:%Y-%m-%d}")
        try:
            week_events = fetch_week(url, week_start, scraper)
        except Exception as exc:
            print(f"  ! Failed to fetch: {exc}")
            continue

        print(f"  -> {len(week_events)} high impact events")
        collected.extend(week_events)
        time.sleep(REQUEST_DELAY_SECONDS)

    return collected


def save_events(events: List[dict], destination: Path) -> None:
    if not events:
        print("No high impact events were captured.")
        return

    destination.parent.mkdir(parents=True, exist_ok=True)
    fieldnames = ["event_id", "date", "time", "currency", "event", "actual", "forecast", "previous"]
    with destination.open("w", newline="", encoding="utf-8") as csv_file:
        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(events)

    print(f"Saved: {len(events)} events to {destination}")


high_impact_events = scrape_high_impact_events()
save_events(high_impact_events, OUTPUT_CSV)
